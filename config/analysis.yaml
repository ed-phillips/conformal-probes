# config/sep_sweep.yaml

models:
  # - meta-llama/Llama-3.1-8B-Instruct
  # - mistralai/Ministral-3-8B-Instruct-2512
  # - Qwen/Qwen3-8B
  - google/gemma-7b-it
  - meta-llama/Llama-3.2-3B-Instruct
  # - HuggingFaceTB/SmolLM3-3B
  - Qwen/Qwen3-4B-Instruct-2507
  - google/gemma-3-4b-it
  - mistralai/Ministral-8B-Instruct-2410

judge_model: meta-llama/Llama-3.2-3B-Instruct

datasets:
  - trivia_qa
  - bioasq
  - medical_o1

generation:
  num_samples: 1000
  num_generations: 10
  num_few_shot: 0
  temperature: 1.0
  model_max_new_tokens: 100
  brief_prompt: default
  metric: hf_judge        # or 'squad' if you want to avoid OpenAI on HPC
  use_context: false

semantic_entropy:
  entailment_model: deberta   # DeBERTa MNLI, fully offline after download
  strict_entailment: true
  use_all_generations: true

probes:
  entropy_key: cluster_assignment_entropy
  token_positions: [tbg, slt]
  train_frac: 0.7     # 70% train
  val_frac: 0.15      # 15% val
  test_frac: 0.15
